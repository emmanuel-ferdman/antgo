# 一站式开发新手教程

## 创建开发环境（深度学习模型开发环境+部署集成环境）
```
# 第一步：生成镜像
# 添加 with-mltalker-user，将创建实验管理服务账号（可选项）
sudo bash docker/build_dev.sh with-android-ndk with-vscode-server

# 第二步：启动IDE环境容器
# -e PASSWORD=123， 在这里可以设置开发者的专有密码
sudo docker run --rm -d --name antgoenvide -p 8080:8080 -e PASSWORD=123 -v /tmp:/tmp -v $(pwd):/workspace -v /var/run/docker.sock:/var/run/docker.sock -v /usr/bin/docker:/usr/bin/docker --gpus all --privileged antgo-env-dev /opt/code-server --host 0.0.0.0 --auth password

```

## 新手COCO目标检测任务
### 数据准备
推荐直接使用本框架共享常规数据集（基于阿里云盘提供支持）。运行如下命令后，将在控制台显示阿里云盘授权二维码，授权后，对应数据集将分享至开发者的阿里云盘内在模型训练时，自动从阿里云盘下载对应数据。
```
antgo share data --name=coco
```

### 创建示例代码
```
antgo create mvp --name=coco
```

### 本地模型训练
```
# 训练完成后，AP(IoU=0.50:0.95)~0.31
# 在./output/coco/output/checkpoint下你将获得checkpoint epoch_60.pth
# 单卡运行
python3 ./coco/main.py --exp=coco --gpu-id=0 --process=train
# 多卡运行
bash launch.sh ./coco/main.py 4 --exp=coco --process=train

```

### 远程模型训练（推送到远程计算服务器）
#### 基于ssh远程任务提交
* 配置ssh远程任务提交

    需要配置远程服务器地址以及用户名（自动配置免密登录）
    ```
    # 第一步：生成ssh配置文件（ssh-submit-config.yaml）
    antgo submitter template --ssh
    # 在创建的ssh-submit-config.yaml文件中填写服务器地址和用户名
    # config:
    #   username: '用户名'
    #   password: ''
    #   ip: 'IP地址'
    # script: 'ssh-submit.sh'

    # 第二步：更新配置并根据提示信息完成免密登录配置
    antgo submitter update --ssh --config=ssh-submit-config.yaml

    # 第三步（可选）：查找所有配置过的远程地址
    antgo submitter ls --ssh

    # 输出，远程地址ip
    # xxx.xxx.xxx.xxx

    # 第四步（可选）：切换到之前配置过的远程地址
    antgo submitter activate --ip=xxx.xxx.xxx.xx --ssh
    ```

* 执行远程提交命令

    执行提交后，任务在远程执行
    ```
    antgo train --ssh --exp=pascal_voc --gpu-id=0,1,2,3
    ```

* 查看远程任务列表
    ```
    antgo ls --ssh
    # 返回, 如下信息
    # running id d1d32ae5291a, name laughing_chaplygin
    ```

* 查看远程任务日志
    ```
    # 运行日志    
    antgo log --ssh --id=d1d32ae5291a
    ```

* 停止远程任务
    ```
    # 停止运行任务
    antgo stop --ssh --id=d1d32ae5291a
    ```

#### 基于k8s远程任务提交
TODO
#### 基于自定义脚本远程任务提交
TODO

### 数据部署
#### 基于ssh远程任务的数据部署
* 部署数据
```
    # 部署 YourDataset.tar，部署后自动解压。在远程任务使用时，通过设置目录/dataset/YourDataset，可以访问数据
    antgo dataset add --src=YourDataset.tar --ssh
```
* 删除数据
```
    # 删除部署的数据
    antgo dataset del --src=YourDataset --ssh
```
* 查看数据
```
    # 查看远程机器的部署数据
    antgo dataset ls --ssh
```

#### 基于k8s远程任务的数据部署
TODO

### 本地模型测试
```
python3 ./coco/main.py --exp=coco --checkpoint=./output/coco/output/checkpoint/epoch_60.pth --process=test
```

### 远程模型测试
#### 基于ssh远程任务提交
```
# 默认已经完成配置ssh远程任务提交
antgo test --exp=coco --gpu-id=0 --checkpoint= --ssh
```
#### 基于k8s远程任务提交
TODO

### 模型导出ONNX
```
# 将输出coco-epoch_60-model.onnx 模型文件
python3 ./coco/main.py --exp=coco --checkpoint=./output/coco/output/checkpoint/epoch_60.pth --process=export
```

## 简洁发布
### 发布模型DEMO
#### 基于DAG引擎创建数据处理管线
```
from antgo.pipeline import *
from antgo.pipeline.functional.data_collection import *
from antgo.pipeline.functional import *
import numpy as np

# 模型输出解码函数
def decode_process_func(
    heatmap_level_1, heatmap_level_2, heatmap_level_3, 
    offset_level_1, offset_level_2, offset_level_3):
    level_stride_list = [8,16,32]
    level_heatmap_list = [heatmap_level_1, heatmap_level_2, heatmap_level_3]
    level_offset_list = [offset_level_1, offset_level_2, offset_level_3]

    all_bboxes = []
    all_labels = []
    for level_i, level_stride in enumerate(level_stride_list):
        level_local_cls = level_heatmap_list[level_i][0,0]  # 仅抽取person cls channel
        level_offset = level_offset_list[level_i]
        
        height, width = level_local_cls.shape
        flatten_local_cls = level_local_cls.flatten()
        topk_inds = np.argsort(flatten_local_cls)[::-1][:100]
        topk_scores = flatten_local_cls[topk_inds]
        pos = np.where(topk_scores > 0.45)
        if pos[0].size == 0:
            continue
        
        topk_inds = topk_inds[pos]
        topk_scores = flatten_local_cls[topk_inds]
        
        topk_inds = topk_inds % (height * width)
        topk_ys = (topk_inds // width).astype(np.float32)
        topk_xs = (topk_inds % width).astype(np.float32)
        
        local_reg = np.transpose(level_offset, [0,2,3,1])   # BxHxWx4
        local_reg = np.reshape(local_reg, [-1,4])
        topk_ltrb_off = local_reg[topk_inds]

        tl_x = (topk_xs * level_stride + level_stride//2 - topk_ltrb_off[:,0] * level_stride)
        tl_y = (topk_ys * level_stride + level_stride//2 - topk_ltrb_off[:,1] * level_stride)
        br_x = (topk_xs * level_stride + level_stride//2 + topk_ltrb_off[:,2] * level_stride)
        br_y = (topk_ys * level_stride + level_stride//2 + topk_ltrb_off[:,3] * level_stride)

        bboxes = np.stack([tl_x,tl_y,br_x,br_y, topk_scores], -1)
        labels =  np.array([0]*bboxes.shape[0])
        all_bboxes.append(bboxes)
        all_labels.append(labels)

    all_bboxes = np.concatenate(all_bboxes, 0)
    all_labels = np.concatenate(all_labels)

    return all_bboxes, all_labels


# 场景1：批量读取本地文件夹图像，处理结果保存到./output/文件夹中
glob['file_path']('./test/*.png').stream(). \
    image_decode['file_path', 'image'](). \
    resize_op['image', 'resized_image'](out_size=(512,384)). \
    preprocess_op['resized_image', 'preprocessed_image'](mean=(128,128,128), std=(128,128,128), permute=[2,0,1], expand_dim=True). \
    inference_onnx_op['preprocessed_image', ('heatmap_level_1', 'heatmap_level_2', 'heatmap_level_3', 'offset_level_1', 'offset_level_2', 'offset_level_3')](onnx_path='coco-epoch_60-model.onnx', input_fields=["image"]). \
    runas_op[('heatmap_level_1', 'heatmap_level_2', 'heatmap_level_3', 'offset_level_1', 'offset_level_2', 'offset_level_3'), ('box', 'label')](func=decode_process_func). \
    nms[('box', 'label'), ('filter_box','filter_label')](iou_thres=0.2). \
    plot_bbox[("resized_image", "filter_box", 'filter_label'), "out"](thres=0.2, color=[[0,0,255]], category_map={'0': 'person'}). \
    image_save['out', 'save'](folder='./output/'). \
    run()


# 场景2：逐帧读取视频，并将结果保存成视频
video_dc['image']('./data.mp4'). \
    resize_op['image', 'resized_image'](out_size=(512,384)). \
    preprocess_op['resized_image', 'preprocessed_image'](mean=(128,128,128), std=(128,128,128), permute=[2,0,1], expand_dim=True). \
    inference_onnx_op['preprocessed_image', ('heatmap_level_1', 'heatmap_level_2', 'heatmap_level_3', 'offset_level_1', 'offset_level_2', 'offset_level_3')](onnx_path='coco-epoch_60-model.onnx', input_fields=["image"]). \
    runas_op[('heatmap_level_1', 'heatmap_level_2', 'heatmap_level_3', 'offset_level_1', 'offset_level_2', 'offset_level_3'), ('box', 'label')](func=decode_process_func). \
    nms[('box', 'label'), ('filter_box','filter_label')](iou_thres=0.2). \
    plot_bbox[("resized_image", "filter_box", 'filter_label'), "out"](thres=0.2, color=[[0,0,255]], category_map={'0': 'person'}). \
    to_video("./output.mp4", width=512, height=384)


```

#### 基于DAG引擎创建WEB DEMO
```
# 创建web上下文（main.py）
# 设置web服务输入占位标识，file_path是输入字段并以此构建处理管线，最后通过select算子搜集需要返回的管线中的信息
# 通过管线叶节点demo的参数input, output分别设置输入和输出的占位标识的类型，前端网页将根据此创建HTML元素。
# 目前支持的前端元素类型包括，['image', 'video', 'text', 'slider', 'checkbox', 'select']
with web['file_path']() as web:
    app=web.image_decode['file_path', 'image'](). \
        resize_op['image', 'resized_image'](out_size=(512,384)). \
        preprocess_op['resized_image', 'preprocessed_image'](mean=(128,128,128), std=(128,128,128), permute=[2,0,1], expand_dim=True). \
        inference_onnx_op['preprocessed_image', ('heatmap_level_1', 'heatmap_level_2', 'heatmap_level_3', 'offset_level_1', 'offset_level_2', 'offset_level_3')](onnx_path='coco-epoch_60-model.onnx', input_fields=["image"]). \
        runas_op[('heatmap_level_1', 'heatmap_level_2', 'heatmap_level_3', 'offset_level_1', 'offset_level_2', 'offset_level_3'), ('box', 'label')](func=decode_process_func). \
        nms[('box', 'label'), ('filter_box','filter_label')](iou_thres=0.2). \
        plot_bbox[("resized_image", "filter_box", 'filter_label'), "out"](thres=0.2, color=[[0,0,255]], category_map={'0': 'person'}). \
        select['out'](). \
        demo(input=[{'data': 'file_path', 'type':'image'}], output=[{'data': 'out', 'type':'image'}])

# 注意：需要通过 uvicorn main:app --reload 启动服务
```

#### 基于DAG引擎创建API SERVER
```
# 创建api服务上下文（main.py）
# 设置api服务输入占位标识，input是输入字段并以此构建处理管线，最后通过select算子搜集需要返回的管线中的信息
# 由于这里通过image_base64_decode来转换接收数据并将其转换成图像，所以这个api服务接受的数据是二进制图像的base64编码
with api['input']() as api:
    app=api.image_base64_decode['input', 'image'](). \
        resize_op['image', 'resized_image'](out_size=(512,384)). \
        preprocess_op['resized_image', 'preprocessed_image'](mean=(128,128,128), std=(128,128,128), permute=[2,0,1], expand_dim=True). \
        inference_onnx_op['preprocessed_image', ('heatmap_level_1', 'heatmap_level_2', 'heatmap_level_3', 'offset_level_1', 'offset_level_2', 'offset_level_3')](onnx_path='coco-epoch_60-model.onnx', input_fields=["image"]). \
        runas_op[('heatmap_level_1', 'heatmap_level_2', 'heatmap_level_3', 'offset_level_1', 'offset_level_2', 'offset_level_3'), ('box', 'label')](func=decode_process_func). \
        nms[('box', 'label'), ('filter_box','filter_label')](iou_thres=0.2). \
        serialize_numpy[('filter_box', 'filter_label'), ('serialize_filter_box', 'serialize_filter_label')](). \
        select['serialize_filter_box', 'serialize_filter_label'](). \
        serve('/coco_demo')

# 注意：需要通过 uvicorn main:app --reload 启动服务
# 现在可以通过post请求，发起服务调用，注意上传的参数
# with open('xxx.png', 'rb') as fp:
#    image_content = fp.read()
#    image_content_base64 = base64.b64encode(image_content)
#
# result = requests.post('http://127.0.0.1:8000/coco_demo',image_content_base64)
```

### 发布模型部署
#### 基于DAG引擎创建算子流水线编译部署包（使用eagleeye扩展包）

部署包编译后，将在当前文件夹下生成deploy/{project}_plugin 部署包
```
# 创建管线并编译
# 注意：计算管线搭建须知
# 支持的管线算子，包括四种类型，
# (1) 用户自定义的C++算子，通过deploy.xxx 引入（xxx为用户实现的c++函数名）
# (2) eagleeye核心算子(来自于eagleeye库)，通过eagleeye.xxx 引入 (xxx为算子名称)
# (3) xxx_op python端算子，通过独立实现c++算子实现一致性
# (4) inference_onnx_op，这是模型预测算子，在c++端已经实现完整的绑定，在编译时，将自动转换成设置的推断引擎

# 注意：编译参数说明 .build(...)函数
# 重要参数，包括
# platform='android/arm64-v8a' or platform='linux/x86-64'
# project_config = {
#            'input': [('image', 'EAGLEEYE_SIGNAL_RGB_IMAGE')], # 指定模块输入信息
#            'output': [
#                ('heatmap_level_1', 'EAGLEEYE_SIGNAL_TENSOR')  # 指定模块输出信息
#            ],
#            'name': 'panda',                                   # 项目名称
#            'git': ''                                          # git地址,指定后将在直接拉去此代码仓库
# }

# 使用snpe预测引擎在高通芯片上部署
import sys
import numpy as np
from antgo.pipeline import *
from antgo.pipeline.extent import op
from antgo.pipeline.functional.data_collection import *
from antgo.pipeline.functional import *

placeholder['image'](np.ones((384,512,3), dtype=np.uint8)). \
    inference_onnx_op[
        'image', ('heatmap_level_1', 'heatmap_level_2', 'heatmap_level_3', 'offset_level_1', 'offset_level_2', 'offset_level_3')
    ](
        onnx_path='/workspace/models/bb-epoch_16-model.onnx', 
        input_fields=["image"], 
        engine='snpe',  # snpe/rknn/tensorrt
        engine_args={
            'alias_output_names':{
                'heatmap_level_1': '/MaxPool',
                'heatmap_level_2': '/MaxPool_1',
                'heatmap_level_3': '/MaxPool_2',
                'offset_level_1': '/bbox_head/reg_head_list.0/reg_head_list.0.5/Relu',
                'offset_level_2': '/bbox_head/reg_head_list.1/reg_head_list.1.5/Relu',
                'offset_level_3': '/bbox_head/reg_head_list.2/reg_head_list.2.5/Relu',
            },
        }). \
    build(
        platform='android/arm64-v8a', 
        project_config={
            'input': [('image', 'EAGLEEYE_SIGNAL_RGB_IMAGE')],
            'output': [
                ('heatmap_level_1', 'EAGLEEYE_SIGNAL_TENSOR')
            ],
            'name': 'panda',
            'git': ''
        }
    )

# 使用rknn预测引擎在rk芯片上运行
placeholder['image'](np.ones((384,512,3), dtype=np.uint8)). \
    inference_onnx_op[
        'image', ('heatmap_level_1', 'heatmap_level_2', 'heatmap_level_3', 'offset_level_1', 'offset_level_2', 'offset_level_3')
    ](
        onnx_path='/workspace/models/bb-epoch_16-model.onnx', 
        mean= [0.5,0.5,0.5],
        std= [1,1,1],
        engine='rknn',
        engine_args={
            'device':'rk3588',
        #   'quantize': True,                                           是否量化
        #   'calibration-images': '/workspace/calibration-images/'      校准文件路径
        }). \
    build(
        platform='android/arm64-v8a',
        project_config={
            'input': [('image', 'EAGLEEYE_SIGNAL_RGB_IMAGE')],
            'output': [
                ('heatmap_level_1', 'EAGLEEYE_SIGNAL_TENSOR')
            ],
            'name': 'panda',
            'git': ''
        }
    )

# 使用tnn预测引擎在移动端gpu运行
placeholder['image'](np.ones((384,512,3), dtype=np.uint8)). \
    inference_onnx_op[
        'image', ('heatmap_level_1', 'heatmap_level_2', 'heatmap_level_3', 'offset_level_1', 'offset_level_2', 'offset_level_3')
    ](
        onnx_path='/workspace/models/bb-epoch_16-model.onnx', 
        mean= [0.5,0.5,0.5],
        std= [1,1,1],
        engine='tnn',
        engine_args={
            'device': 'GPU'
        }). \
    build(
        platform='android/arm64-v8a',
        project_config={
            'input': [('image', 'EAGLEEYE_SIGNAL_RGB_IMAGE')],
            'output': [
                ('heatmap_level_1', 'EAGLEEYE_SIGNAL_TENSOR')
            ],
            'name': 'panda',
            'git': ''
        }
    )

# 使用tensorrt预测引擎在nvidia gpu上运行
placeholder['image'](np.ones((384,512,3), dtype=np.uint8)). \
    inference_onnx_op[
        'image', ('heatmap_level_1', 'heatmap_level_2', 'heatmap_level_3', 'offset_level_1', 'offset_level_2', 'offset_level_3')
    ](
        onnx_path='/workspace/models/bb-epoch_16-model.onnx', 
        mean= [0.5,0.5,0.5],
        std= [1,1,1],
        engine='tensorrt'). \
    build(
        platform='linux/x86-64',
        project_config={
            'input': [('image', 'EAGLEEYE_SIGNAL_RGB_IMAGE')],
            'output': [
                ('heatmap_level_1', 'EAGLEEYE_SIGNAL_TENSOR')
            ],
            'name': 'panda',
            'git': ''
        }
    )

```

#### 部署包运行测试
运行如下脚本，执行部署包
```
from antgo.pipeline import *
project_kwargs = {
    'image': np.ones((384,512,3), dtype=np.uint8)
}
run(project='panda', **project_kwargs)
```

## 相关文档
![]()

## 常见错误信息汇总
### 使用C++扩展时报/miniconda3/bin/../lib/libstdc++.so.6: version `GLIBCXX_3.4.30' not found

这是由于conda环境的libstd库与编译c++库不符导致，可以通过如下方式解决
```
rm /miniconda3/bin/../lib/libstdc++.so.6
ln /usr/lib/x86_64-linux-gnu/libstdc++.so.6 /miniconda3/bin/../lib/libstdc++.so.6
```