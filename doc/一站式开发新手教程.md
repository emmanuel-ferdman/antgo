# 一站式开发新手教程

## 创建开发环境（深度学习模型开发环境+部署集成环境）
```
# 第一步：生成镜像
# 添加 with-mltalker-user，将创建实验管理服务账号（可选项）
bash docker/build.sh with-vscode-server

# 第二步：启动IDE环境容器
# -e PASSWORD=123， 在这里可以设置开发者的专有密码
sudo docker run --rm -d -e PASSWORD=123 --name mycodeserver --network host --gpus all antgo-env /opt/code-server --host 0.0.0.0 --auth password

```

## 新手COCO目标检测任务
### 数据准备
推荐直接使用本框架共享常规数据集（基于阿里云盘提供支持）。运行如下命令后，将在控制台显示阿里云盘授权二维码，授权后，对应数据集将分享至开发者的阿里云盘内在模型训练时，自动从阿里云盘下载对应数据。
```
antgo share data --name=coco
```

### 创建示例代码
```
antgo create mvp --name=coco
```

### 模型训练
```
# 训练完成后，AP(IoU=0.50:0.95)~0.31
# 在./output/coco/output/checkpoint下你将获得checkpoint epoch_60.pth
# 单卡运行
python3 ./coco/main.py --exp=coco --gpu-id=0 --process=train
# 多卡运行
bash launch.sh ./coco/main.py 4 --exp=coco --process=train

```
### 模型测试
```
python3 ./coco/main.py --exp=coco --checkpoint=./output/coco/output/checkpoint/epoch_60.pth --process=test
```

### 模型导出ONNX
```
# 将输出coco-epoch_60-model.onnx 模型文件
python3 ./coco/main.py --exp=coco --checkpoint=./output/coco/output/checkpoint/epoch_60.pth --process=export
```

## 简洁发布
### 发布模型DEMO
#### 基于DAG引擎创建数据处理管线
```
from antgo.pipeline import *
from antgo.pipeline.functional.data_collection import *
from antgo.pipeline.functional import *
import numpy as np

# 模型输出解码函数
def decode_process_func(
    heatmap_level_1, heatmap_level_2, heatmap_level_3, 
    offset_level_1, offset_level_2, offset_level_3):
    level_stride_list = [8,16,32]
    level_heatmap_list = [heatmap_level_1, heatmap_level_2, heatmap_level_3]
    level_offset_list = [offset_level_1, offset_level_2, offset_level_3]

    all_bboxes = []
    all_labels = []
    for level_i, level_stride in enumerate(level_stride_list):
        level_local_cls = level_heatmap_list[level_i][0,0]  # 仅抽取person cls channel
        level_offset = level_offset_list[level_i]
        
        height, width = level_local_cls.shape
        flatten_local_cls = level_local_cls.flatten()
        topk_inds = np.argsort(flatten_local_cls)[::-1][:100]
        topk_scores = flatten_local_cls[topk_inds]
        pos = np.where(topk_scores > 0.45)
        if pos[0].size == 0:
            continue
        
        topk_inds = topk_inds[pos]
        topk_scores = flatten_local_cls[topk_inds]
        
        topk_inds = topk_inds % (height * width)
        topk_ys = (topk_inds // width).astype(np.float32)
        topk_xs = (topk_inds % width).astype(np.float32)
        
        local_reg = np.transpose(level_offset, [0,2,3,1])   # BxHxWx4
        local_reg = np.reshape(local_reg, [-1,4])
        topk_ltrb_off = local_reg[topk_inds]

        tl_x = (topk_xs * level_stride + level_stride//2 - topk_ltrb_off[:,0] * level_stride)
        tl_y = (topk_ys * level_stride + level_stride//2 - topk_ltrb_off[:,1] * level_stride)
        br_x = (topk_xs * level_stride + level_stride//2 + topk_ltrb_off[:,2] * level_stride)
        br_y = (topk_ys * level_stride + level_stride//2 + topk_ltrb_off[:,3] * level_stride)

        bboxes = np.stack([tl_x,tl_y,br_x,br_y, topk_scores], -1)
        labels =  np.array([0]*bboxes.shape[0])
        all_bboxes.append(bboxes)
        all_labels.append(labels)

    all_bboxes = np.concatenate(all_bboxes, 0)
    all_labels = np.concatenate(all_labels)

    return all_bboxes, all_labels


# 场景1：批量读取本地文件夹图像，处理结果保存到./output/文件夹中
glob['file_path']('./test/*.png').stream(). \
    image_decode['file_path', 'image'](). \
    resize_op['image', 'resized_image'](size=(512,384)). \
    preprocess_op['resized_image', 'preprocessed_image'](mean=(128,128,128), std=(128,128,128), permute=[2,0,1], expand_dim=True). \
    inference_onnx_op['preprocessed_image', ('heatmap_level_1', 'heatmap_level_2', 'heatmap_level_3', 'offset_level_1', 'offset_level_2', 'offset_level_3')](onnx_path='coco-epoch_60-model.onnx', input_fields=["image"]). \
    runas_op[('heatmap_level_1', 'heatmap_level_2', 'heatmap_level_3', 'offset_level_1', 'offset_level_2', 'offset_level_3'), ('box', 'label')](func=decode_process_func). \
    nms[('box', 'label'), ('filter_box','filter_label')](iou_thres=0.2). \
    plot_bbox[("resized_image", "filter_box", 'filter_label'), "out"](thres=0.2, color=[[0,0,255]], category_map={'0': 'person'}). \
    image_save['out', 'save'](folder='./output/'). \
    run()


# 场景2：逐帧读取视频，并将结果保存成视频
video_dc['image']('./data.mp4'). \
    resize_op['image', 'resized_image'](size=(512,384)). \
    preprocess_op['resized_image', 'preprocessed_image'](mean=(128,128,128), std=(128,128,128), permute=[2,0,1], expand_dim=True). \
    inference_onnx_op['preprocessed_image', ('heatmap_level_1', 'heatmap_level_2', 'heatmap_level_3', 'offset_level_1', 'offset_level_2', 'offset_level_3')](onnx_path='coco-epoch_60-model.onnx', input_fields=["image"]). \
    runas_op[('heatmap_level_1', 'heatmap_level_2', 'heatmap_level_3', 'offset_level_1', 'offset_level_2', 'offset_level_3'), ('box', 'label')](func=decode_process_func). \
    nms[('box', 'label'), ('filter_box','filter_label')](iou_thres=0.2). \
    plot_bbox[("resized_image", "filter_box", 'filter_label'), "out"](thres=0.2, color=[[0,0,255]], category_map={'0': 'person'}). \
    to_video("./output.mp4", width=512, height=384)


```

#### 基于DAG引擎创建WEB DEMO
```
# 创建web上下文（main.py）
# 设置web服务输入占位标识，file_path是输入字段并以此构建处理管线，最后通过select算子搜集需要返回的管线中的信息
# 通过管线叶节点demo的参数input, output分别设置输入和输出的占位标识的类型，前端网页将根据此创建HTML元素。
# 目前支持的前端元素类型包括，['image', 'video', 'text', 'slider', 'checkbox', 'select']
with web['file_path']() as web:
    app=web.image_decode['file_path', 'image'](). \
        resize_op['image', 'resized_image'](size=(512,384)). \
        preprocess_op['resized_image', 'preprocessed_image'](mean=(128,128,128), std=(128,128,128), permute=[2,0,1], expand_dim=True). \
        inference_onnx_op['preprocessed_image', ('heatmap_level_1', 'heatmap_level_2', 'heatmap_level_3', 'offset_level_1', 'offset_level_2', 'offset_level_3')](onnx_path='coco-epoch_60-model.onnx', input_fields=["image"]). \
        runas_op[('heatmap_level_1', 'heatmap_level_2', 'heatmap_level_3', 'offset_level_1', 'offset_level_2', 'offset_level_3'), ('box', 'label')](func=decode_process_func). \
        nms[('box', 'label'), ('filter_box','filter_label')](iou_thres=0.2). \
        plot_bbox[("resized_image", "filter_box", 'filter_label'), "out"](thres=0.2, color=[[0,0,255]], category_map={'0': 'person'}). \
        select['out'](). \
        demo(input=[{'data': 'file_path', 'type':'image'}], output=[{'data': 'out', 'type':'image'}])

# 注意：需要通过 uvicorn main:app --reload 启动服务
```

#### 基于DAG引擎创建API SERVER
```
# 创建api服务上下文（main.py）
# 设置api服务输入占位标识，input是输入字段并以此构建处理管线，最后通过select算子搜集需要返回的管线中的信息
# 由于这里通过image_base64_decode来转换接收数据并将其转换成图像，所以这个api服务接受的数据是二进制图像的base64编码
with api['input']() as api:
    app=api.image_base64_decode['input', 'image'](). \
        resize_op['image', 'resized_image'](size=(512,384)). \
        preprocess_op['resized_image', 'preprocessed_image'](mean=(128,128,128), std=(128,128,128), permute=[2,0,1], expand_dim=True). \
        inference_onnx_op['preprocessed_image', ('heatmap_level_1', 'heatmap_level_2', 'heatmap_level_3', 'offset_level_1', 'offset_level_2', 'offset_level_3')](onnx_path='coco-epoch_60-model.onnx', input_fields=["image"]). \
        runas_op[('heatmap_level_1', 'heatmap_level_2', 'heatmap_level_3', 'offset_level_1', 'offset_level_2', 'offset_level_3'), ('box', 'label')](func=decode_process_func). \
        nms[('box', 'label'), ('filter_box','filter_label')](iou_thres=0.2). \
        serialize_numpy[('filter_box', 'filter_label'), ('serialize_filter_box', 'serialize_filter_label')](). \
        select['serialize_filter_box', 'serialize_filter_label'](). \
        serve('/coco_demo')

# 注意：需要通过 uvicorn main:app --reload 启动服务
# 现在可以通过post请求，发起服务调用，注意上传的参数
# with open('xxx.png', 'rb') as fp:
#    image_content = fp.read()
#    image_content_base64 = base64.b64encode(image_content)
#
# result = requests.post('http://127.0.0.1:8000/coco_demo',image_content_base64)
```

### 发布模型部署
#### 基于DAG引擎创建算子流水线（使用eagleeye扩展包）

#### 发布部署包
##### SDK部署包

##### 云端部署包
